# Ch·ªçn ng√¥n ng·ªØ:
language = "vi" 
input_text ="Xin ch√†o, t√¥i l√† m·ªôt c√¥ng c·ª• c√≥ kh·∫£ nƒÉng chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh gi·ªçng n√≥i t·ª± nhi√™n, ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi nh√≥m N√≥n l√°. T√¥i c√≥ th·ªÉ h·ªï tr·ª£ ng∆∞·ªùi khi·∫øm th·ªã,  ƒë·ªçc s√°ch n√≥i, l√†m tr·ª£ l√Ω ·∫£o, review phim, l√†m waifu ƒë·ªÉ an ·ªßi b·∫°n, v√† ph·ª•c v·ª• nhi·ªÅu m·ª•c ƒë√≠ch kh√°c." # @param {type:"string"}
#  T·ª± ƒë·ªông chu·∫©n h√≥a ch·ªØ (VD: 20/11 -> hai m∆∞∆°i th√°ng m∆∞·ªùi m·ªôt)
normalize_text = True 
# In chi ti·∫øt x·ª≠ l√Ω
verbose = True 

def cry_and_quit():
    print("> L·ªói r·ªìi huhu üò≠üò≠, b·∫°n h√£y nh·∫•n ch·∫°y l·∫°i ph·∫ßn n√†y nh√©!")
    quit()

import os
import string
import unicodedata
from datetime import datetime
from pprint import pprint

import torch
import torchaudio
from tqdm import tqdm
from underthesea import sent_tokenize, word_tokenize
from unidecode import unidecode

try:
    from vinorm import TTSnorm
    from TTS.TTS.tts.configs.xtts_config import XttsConfig
    from TTS.TTS.tts.models.xtts import Xtts
except:
    cry_and_quit()

# Load model
def clear_gpu_cache():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def load_model(xtts_checkpoint, xtts_config, xtts_vocab):
    clear_gpu_cache()
    if not xtts_checkpoint or not xtts_config or not xtts_vocab:
        return "You need to run the previous steps or manually set the `XTTS checkpoint path`, `XTTS config path`, and `XTTS vocab path` fields !!"
    config = XttsConfig()
    config.load_json(xtts_config)
    XTTS_MODEL = Xtts.init_from_config(config)
    print("Loading XTTS model! ")
    XTTS_MODEL.load_checkpoint(config,
                               checkpoint_path=xtts_checkpoint,
                               vocab_path=xtts_vocab,
                               use_deepspeed=True)
    if torch.cuda.is_available():
        XTTS_MODEL.cuda()

    print("Model Loaded!")
    return XTTS_MODEL


def get_file_name(text, max_char=50):
    filename = text[:max_char]
    filename = filename.lower()
    filename = filename.replace(" ", "_")
    filename = filename.translate(str.maketrans("", "", string.punctuation.replace("_", "")))
    filename = unidecode(filename)
    current_datetime = datetime.now().strftime("%m%d%H%M%S")
    filename = f"{current_datetime}_{filename}"
    return filename


def calculate_keep_len(text, lang):
    if lang in ["ja", "zh-cn"]:
        return -1

    word_count = len(text.split())
    num_punct = (
        text.count(".")
        + text.count("!")
        + text.count("?")
        + text.count(",")
    )

    if word_count < 5:
        return 15000 * word_count + 2000 * num_punct
    elif word_count < 10:
        return 13000 * word_count + 2000 * num_punct
    return -1


def normalize_vietnamese_text(text):
    text = (
        TTSnorm(text, unknown=False, lower=False, rule=True)
        .replace("-", "")
        .replace("...", ".")
        .replace("..", ".")
        .replace(":.", ".")
        .replace("!.", "!")
        .replace("?.", "?")
        .replace(" .", ".")
        .replace(" ,", ",")
        .replace('"', "")
        .replace("'", "")
        .replace("  ", " ")
        .replace("AI", "√Çy Ai")
        .replace("A.I", "√Çy Ai")
    )
    return text

def merge_short_sentences(text):
    sentences = sent_tokenize(text)
    merged_sentences = []
    i = 0

    while i < len(sentences):
        words = word_tokenize(sentences[i])
        # Ki·ªÉm tra s·ªë l∆∞·ª£ng t·ª´ trong c√¢u
        if len(words) < 10 and i+1 < len(sentences):
            # G·ªôp c√¢u ng·∫Øn v·ªõi c√¢u k·∫ø ti·∫øp
            sentences[i+1] = sentences[i][:-1] + ', ' + sentences[i+1]
        else:
            # N·∫øu c√¢u kh√¥ng ng·∫Øn, th√™m n√≥ v√†o danh s√°ch c√°c c√¢u ƒë√£ g·ªôp
            merged_sentences.append(sentences[i])
        i += 1

    return merged_sentences


def run_tts(XTTS_MODEL, lang, tts_text, speaker_audio_file,
            normalize_text= True,
            verbose=False):
    """
    Run text-to-speech (TTS) synthesis using the provided XTTS_MODEL.

    Args:
        XTTS_MODEL: A pre-trained TTS model.
        lang (str): The language of the input text.
        tts_text (str): The text to be synthesized into speech.
        speaker_audio_file (str): Path to the audio file of the speaker to condition the synthesis on.
        normalize_text (bool, optional): Whether to normalize the input text. Defaults to True.
        verbose (bool, optional): Whether to print verbose information. Defaults to False.
        output_chunks (bool, optional): Whether to save synthesized speech chunks separately. Defaults to False.

    Returns:
        str: Path to the synthesized audio file.
    """

    if XTTS_MODEL is None or not speaker_audio_file:
        return "You need to run the previous step to load the model !!", None, None

    output_dir = "./output"
    os.makedirs(output_dir, exist_ok=True)

    gpt_cond_latent, speaker_embedding = XTTS_MODEL.get_conditioning_latents(
        audio_path=speaker_audio_file,
        gpt_cond_len=XTTS_MODEL.config.gpt_cond_len,
        max_ref_length=XTTS_MODEL.config.max_ref_len,
        sound_norm_refs=XTTS_MODEL.config.sound_norm_refs,
    )

    if normalize_text and lang == "vi":
        # Bug on google colab
        try:
            tts_text = normalize_vietnamese_text(tts_text)
        except:
            cry_and_quit()

    if lang in ["ja", "zh-cn"]:
        tts_texts = tts_text.split("„ÄÇ")
    else:
        tts_texts = merge_short_sentences(tts_text)

    if verbose:
        print("Text for TTS:")
        pprint(tts_texts)

    wav_chunks = []
    for text in tqdm(tts_texts):
        if text.strip() == "":
            continue

        wav_chunk = XTTS_MODEL.inference(
            text=text,
            language=lang,
            gpt_cond_latent=gpt_cond_latent,
            speaker_embedding=speaker_embedding,
            temperature=0.3,
            length_penalty=1.0,
            repetition_penalty=10.0,
            top_k=30,
            top_p=0.85,
        )

        # Quick hack for short sentences
        keep_len = calculate_keep_len(text, lang)
        wav_chunk["wav"] = torch.tensor(wav_chunk["wav"][:keep_len])

        wav_chunks.append(wav_chunk["wav"])

    out_wav = torch.cat(wav_chunks, dim=0).unsqueeze(0)
    out_path = os.path.join(output_dir, f"{get_file_name(tts_text)}.wav")
    torchaudio.save(out_path, out_wav, 24000)

    if verbose:
        print(f"Saved final file to {out_path}")

    return out_path

print("> ƒêang n·∫°p m√¥ h√¨nh...")
vixtts_model = load_model(xtts_checkpoint="model/model.pth",
                        xtts_config="model/config.json",
                        xtts_vocab="model/vocab.json")
print("> ƒê√£ n·∫°p m√¥ h√¨nh")

while True:
  voice = int(input("Ch·ªçn gi·ªçng ƒë·ªçc [1, 2, 3]:"))
  if voice == 1:
    reference_audio = "voice/voice1.wav"
  elif voice == 2:
    reference_audio = "voice/voice2.wav"
  elif voice == 3:
    reference_audio = "voice/voice3.wav"
  else:
    print("L·ª±a ch·ªçn kh√¥ng ph√π h·ª£p. H√£y ch·ªçn l·∫°i.")
    continue
  with open('input.txt', encoding='utf-8') as f:
    input_text = f.read()
  print("> ƒêang chuy·ªÉn ƒë·ªïi...")
  audio_file = run_tts(vixtts_model,
          lang=language,
          tts_text=input_text,
          speaker_audio_file=reference_audio,
          normalize_text=normalize_text,
          verbose=verbose)
  print("> ƒê√£ chuy·ªÉn ƒë·ªïi xong!")
  if input("B·∫°n c√≥ mu·ªën ti·∫øp t·ª•c kh√¥ng [1:C√≥, 0:Kh√¥ng]") != '1':
    break
